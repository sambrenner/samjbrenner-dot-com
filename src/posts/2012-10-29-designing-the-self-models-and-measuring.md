---
title: 'Designing the Self: Models and Measuring'
author: sambrenner
type: post
date: 2012-10-29T03:15:51+00:00
draft: true
private: true
path: /notes/designing-the-self-models-and-measuring/
categories:
  - ITP
  - DIY Health
---
My final project in DIY Health will be a system whose goal is to help less emotionally-aware individuals (such as myself) to identify, define and understand their emotions. This week I&#8217;m exploring similar systems, sharpening my own and considering how it will work.
<!--more-->
## My System
My goal, as stated above, stems from my belief that being aware of emotions and being able to recognize them goes a long way towards achieving peace of mind and emotional stability. My system will work by asking the user through their smartphone to input what they perceive to be their current emotional state (sensor). It will present the user with other options to consider. It will learn from the user to best understand their own emotional trends, yet it will include a degree of randomness to ensure the system isn&#8217;t simply telling the user how they feel (comparator). It will offer definitions for all presented emotions, offer suggestions on how to manage those emotions and  also log emotions over time and display them back to the user (actuator). By doing all of these things, they system will allow users to better understand their emotional state and act on emotions as they bubble up into consciousness.
## Competitive Analysis
Current tools for emotion tracking fall in to two categories: those that require the user to make judgments on their emotions and those that attempt to infer emotions from another form of input. One example of a tool that asks for user input is [Mappiness][1], an iPhone app. Mappiness is the embodiment of a system whose goal is to map happiness across the UK, and it achieves this by asking users to rate feelings like happiness, relaxation and awakedness using a slider.
<img class="aligncenter size-full wp-image-180" title="mzl.yvbpgvkp.320x480-75" src="/img/uploads/2012/10/mzl.yvbpgvkp.320x480-75.jpeg" alt="" width="270" height="480" /> 
<img title="mzl.yxcjytzw.320x480-75" src="/img/uploads/2012/10/mzl.yxcjytzw.320x480-75.jpeg" alt="" width="270" height="480" />
The app pings users once or more times a day to ask them to analyze their current happiness this way. The output of the system is a visual representation of the user&#8217;s happiness over time. I don&#8217;t have an iPhone so I can&#8217;t fully explore its features &#8211; screenshots of the app suggest it also has a component to track with whom the user is happiest around, which is an interesting way to give the user feedback.
For their purposes, the sliders work well as a form of input. My system might not benefit from using this method, since Mappiness is only concerned with three easily-understood emotions and I will be working with more complex, harder-to-define emotions. Mappiness&#8217;s line graph output doesn&#8217;t provide a great deal of feedback to the user. Time, as opposed to location or social activity, might not be the best measurement to put happiness against. My system&#8217;s main output will be the definition of the emotion, but I would also like to provide some sort of visualization as a secondary output. &#8220;Mood vs time&#8221; might be an option but its meaning is limited unless the user can also see what about the time causes shifts in mood. If my happiness spikes at 5 PM, why is that? I think a successful visual output should answer that question. Mappiness also has bigger-picture visualizations ([maps][2] and [graphs][3]) that, while they aren&#8217;t a part of the individual user&#8217;s system, suffer from the same downfall. I can see that the UK&#8217;s mood rose steadily a few days ago, but why was that?
An alternate approach to measuring emotions is through physical inference. [Affectiva][4], a company spun out of MIT&#8217;s Media Lab, uses a combination of galvanic skin response and facial tracking to take an educated guess as to the user&#8217;s mood.
I like the idea of representing emotions on a two dimensional graph (positive/negative vs excited/calm). But for my system to be successful, I want the user to explore the definitions of emotions to make their own judgement on how they feel. Having a computer tell the user that it thinks they feel anxious would not achieve this goal. For my system, I think user input is a must-have. Also I&#8217;m not sure how much I trust technology like this. First of all, facial tracking isn&#8217;t viable for long-term emotional tracking as it requires a camera pointed at the user. Secondly, facial tracking can&#8217;t be all that reliable. I know personally that the emotion I convey to others through my face isn&#8217;t necessarily how I&#8217;m actually feeling. Culturally, there&#8217;s plenty to back this up: the idea of a &#8220;game face&#8221; or the saying &#8220;turn that frown upside-down!&#8221; show that facial expressions for the benefit of others as much as they are for you.
## Methods of Measurement
I wanted to understand how to best categorize emotions before understanding them. Some research led me to this [tree diagram][5] from W. Parrott&#8217;s _Emotions in Social Psychology_. He proposes six primary emotions: love, joy, surprise, anger, sadness and fear. Each has a set of secondary and tertiary emotions associated with it. With this, I&#8217;m considering three ways to measure emotions.
The first way would be true to the tree nature of Parrott&#8217;s classification. The six primary emotions would be listed. Selecting on one would expose its secondary emotions. Selecting one of those would expose the tertiary emotions. At any point along the way, the user would be able to select a listed emotion as the one that they are currently feeling. It is very linear and open-ended, allowing the user to learn definitions for as many emotions as possible and not hiding any options.
The second way is the graph as shown in the Affectiva video. Users would estimate where they currently fall on the graph, where the X axis is negative to positive, and the Y axis is calm to excited. When they select a location, a list of corresponding emotions shows up, and the user selects a choice from there. It presents a good degree of variety but immediately lets the user narrow down their choices, making the emotion choosing process easier.
The third way would be a randomly generated word cloud of emotions. The user picks which one is closest to their state and the cloud is refreshed to show still random, but more similar emotions. This is repeated until the user selects an emotion. It has the ability to learn from the user and tailor the initial word cloud to emotions they generally feel at the chosen time/location. It would need to contain an element of randomness though.
Another thing to consider is when to measure. I think a good opportunity to do this would be at a phone&#8217;s lock screen. While the iPhone doesn&#8217;t allow for modification of the lock screen, it is available on Android. At random points throughout the day, users would be asked to complete the emotional recognition process instead of entering their phone&#8217;s password as they normally do. Push notifications is an obvious (yet effective) choice as well. A more novel approach might be to replace ads on other apps and websites with reminders, though that might be too easily ignored.

 [1]: http://www.mappiness.org.uk/
 [2]: http://www.mappiness.org.uk/maps/
 [3]: http://www.mappiness.org.uk/meters/
 [4]: http://www.affectiva.com/
 [5]: http://books.google.com/books?id=jV5QVgM6Me8C&lpg=PP1&pg=PA34#v=onepage&q&f=false
